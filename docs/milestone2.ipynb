{"cells":[{"cell_type":"markdown","source":"## Introduction\n\nDifferentiation is important to computation, optimization and engineering, showing up everywhere from neural networks to physics equations and any field that requires the calculation of rate of change, extrema, and zeros of a function. Automatic differentiation (autodiff) allows for the automatic computation of precise derivatives of a values by applying the chain rule to  a sequence of elementary arithmetic operations and functions repeatedly. \n\n### Automatic differentiation differs from the finite difference method and symbolic method of differentiation. \n\n**Symbolic differtiation** find the derivative of a given forumula, gives a new forumula as an output and plugs in a value to find the derivative at that value. \n\nFor example to find the derivative of f where\n$$f\\left(x\\right) = x^3 + 3.$$\n\nWe get \n\n$$\\dfrac{d}{dx} f\\left(x\\right)= \\dfrac{d}{dx}x^3 + \\dfrac{d}{dx}3 .$$\n\nIf we combine derivative constant rule and power rule we get \n\n$$\\dfrac{d}{dx} f\\left(x\\right)= 3x^2 + 0 =  3x^2.$$\n\nThis allows calculation at machine precision and provides a solution to a class of problems, not just a single problems. However, this can lead to inefficent code and can be costly to evalute. \n\n**Finite difference method** estimates a derivative by computing the slope of a secant line through the points (x, f(x)) and (x + h, f(x + h)), choosing a small number for h. The slope of this secant line approaches the slope of the tangent line as h approaches zero.\n<img src=\"/home/jovyan/work/image-20201018-153111.png\" width=\"400\">\n\nTherefore the derivative of f at x is:\n$$ f'(x) = \\lim_{h \\to 0}\\dfrac{f\\left(x+h\\right) - f\\left(x\\right)}{h}$$\n\nThis aproach is quick and easy but suffers from accuracy and precision due to truncation and rounding errors. \n\n**Automatic Differentiation** is more precise than the finite differences method and more efficient than symbolic differentiation. It allows for the computation of the derivative to machine precision without forming the formula for the derivative by using the chain rule to decompose derivatives. \n\n $$y = f\\left(g\\left(h\\left(x\\right)\\right)\\right) = f\\left(g\\left(h\\left(w_{0}\\right)\\right)\\right) = f\\left(g\\left(w_{1}\\right)\\right) = f\\left(w_{2})\\right) = w_{3}$$\n \n $$w_{0}= x$$\n\n $$w_{1}= h\\left(w_{0}\\right)$$\n\n $$w_{2}= g\\left(w_{1}\\right)$$\n \n $$w_{3}= f\\left(w_{2}\\right)=y$$ \n \n The Chain rule gives\n \n $\n  \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial w_{2}}\\frac{\\partial w_{2}}{\\partial w_{1}}\\frac{\\partial w_{1}}{\\partial x}=\\frac{\\partial f(w_{2})}{\\partial w_{2}}\\frac{\\partial g(w_{1})}{\\partial w_{1}}\\frac{\\partial h(w_{0})}{\\partial x}.$","metadata":{"deepnote_cell_type":"markdown","tags":[],"output_cleared":false,"cell_id":"00000-65cfbae6-1fcb-4ba4-98fc-141176907b4a"}},{"cell_type":"markdown","source":"## Background\n### Automatic Differntiation: the forward mode\n#### Review of the Chain Rule\nThe chain rule allows for computing the derivative of a composite funtion. If f and g are both differentiable then \nthe chain rule gives the derivative of $f(g(x))$ in terms of the derivatives of f and g, $$f'(g(x))g'(x)$$.\n\nThe derivative is $$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial g}\\dfrac{\\partial g}{\\partial x}$$\n\n##### Example: $f\\left(g\\left(x\\right)\\right) = \\ln\\left(x\\right)^7$\n$$\\dfrac{\\partial f}{\\partial g} = 7\\left(g\\right)^6, \\quad \\dfrac{\\partial g}{\\partial x} = \\dfrac{1}{x},\\quad \\Rightarrow \\quad \\dfrac{\\partial f}{\\partial x} = 7\\ln\\left(x\\right)^6*\\dfrac{1}{x}.$$\n\n#### Elementary Functions\nComplex functions can be broken down into simpler paired functions which can be applied to the chain rule. These elementary operations include the arithmetic operations (addition, subtraction, multiplication and division) and exponential and trigonometric functions whose derivatives we know. We can combine these elementary functions to make more complex functions and find the derivatives of these more complex functions with the chain rule. \n\n#### The Gradient\nWe can find the derivative of a function with multiple inputs by applying the chain rule. The derivative of $f(x) = g(u(x), v(x))$ is \n\n$  \\frac{\\partial f}{\\partial x} = \\frac{\\partial g}{\\partial u}\\frac{\\partial u}{\\partial x} + \\frac{\\partial g}{\\partial v}\\frac{\\partial v}{\\partial x}.$\n\nWe can write this as \n\n$  \\nabla_{x}g = \\sum_{i=1}^{n}{\\frac{\\partial g}{\\partial y_{i}}\\nabla y_{i}\\left(x\\right)}.$\n\nWith this formula we can find the partial derivatives for each input. \n\n#### Computational Trace\nWe can compute the derivative of elementary functions and combine them using the chain rule to find the derivative of more complex functions. \n\nConsider the following function $$ f\\left(x,y\\right) = \\exp\\left(-\\left(\\sin\\left(x\\right) - \\cos\\left(y\\right)\\right)^{2}\\right)$$  We'd like to evalute $f$ at the point $x= \\left(\\dfrac{\\pi}{2}, \\dfrac{\\pi}{3}\\right)$.\n\n\n| Trace |  Eleme Operation  |  Numerical Value  |  Elem Deriv  |  value in respect to x  |  value in respect to y  |\n| :------: | :----------------------: | :------------------------------: | :------: | :------: | :------: |\n| $x_{1}$ | x | $\\pi/2$ | x1. | 1 | 0 |\n| $x_{2}$ | y | $\\pi/3$ | x2. | 0 | 1 |\n| $v_{1}$ | $\\sin(x1)$ | 1 | $\\cos(x1)*\\dot{x}1$ | 0 | 0 |\n| $v_{2}$ | cos(x2) | 0.5 | $\\sin(x2)*\\dot{x}_{2}$ | 0 | $\\sqrt{3}/2$ |\n| ${v}_{3}$ | $v_{1} - v_{2}$ | 0.5 |  $\\dot{v}_{1} - \\dot{v}_{2}$ | 0 | $\\sqrt{3}/2$  |\n| ${v}_{4}$ | ${v}_{3}^2$ | 1/4 | $2{v}_{3}  \\dot{v}_{3}$ |0  |$\\sqrt{3}/2$   | \n| ${v}_{5}$ | $-{v}_{4}$ | -1/4 | $-\\dot{v}_{4}$ | 0 |- $\\sqrt{3}/2$ | \n| ${v}_{6}$| $exp({v}_{5})$ | $exp(-1/4)$ | $exp({v}_{5})*\\dot{v}_{5}.$  | 0 | $\\exp(-1/4)$ * $(-\\sqrt{3}/2)$  | \n\n#### The Forward mode\nWe work from the inside out, starting with what we want to evaluate \n$x= \\left(\\dfrac{\\pi}{2}, \\dfrac{\\pi}{3}\\right)$, then build out the actual function at each step subsituting the derivative of the inner functions  in the chain rule. \n The function f(x,y) is composed of elementary functions which we know the derivative of, we compute the derivative of the elementary functions then \n use the chain rule to build up to the larger function.\n\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"output_cleared":false,"cell_id":"00001-ee1c0405-c4e6-4303-acd1-175db0655961"}},{"cell_type":"markdown","source":"# How to Use DeriveMeCrazy Auto_diff:\n## Importing: \n\n#### Direct import:\nUsers will be able to clone the project off of github and run it locally.\nOnce they have the code in their local env, they will be able to import the different classes implemented. \n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"output_cleared":false,"cell_id":"00004-88d55838-8c26-4be9-9acb-5bed8de0652d"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"output_cleared":true,"source_hash":null,"execution_start":1603401670993,"execution_millis":3056,"cell_id":"00005-3cb8397c-a83c-4ae6-957d-90151722c04a"},"source":"pip install dmc_auto_diff","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Demo\n\nWe include a basic demo below, including how to import, create an object, and use `AutoDiff` for calculating derivatives in an arithmetic operation and a trignometric operation. A more detailed demo for more types of operations is included in `docs/demo.ipynb`.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00004-ce206903-22cb-42bc-9383-9fc1fa1e28ff"}},{"cell_type":"markdown","source":"#### Import","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00005-07b8abd4-a8f6-4028-8d15-67e47a895240"}},{"cell_type":"code","source":"import auto_diff_pkg.AutoDiff as AD","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-4abae248-4140-4aa9-8157-bcbc71a00ba8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Creating an `AutoDiff` object","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00007-b1cf517f-e40c-4995-82eb-dbf1fd808914"}},{"cell_type":"code","source":"ad1 = AD.AutoDiff(5.0)\nad2 = AD.AutoDiff(3.0)","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00007-72a8766f-5841-4ea5-a5bd-fb864affd6f4"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Arithmetic operations","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00009-3b2c1453-c028-4e50-8c36-745c41ca6b3c"}},{"cell_type":"code","source":"ad3 = ad1 * ad2\n\nprint('value: {}'.format(ad3.val))\nprint('derivative: {}'.format(ad3.der))","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00010-84f592ae-e02c-49fb-88c0-ec6982e48d06"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Trignometric operations","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00011-029f3a69-e72e-4c1d-8e4a-9a5812c6f9c9"}},{"cell_type":"code","source":"ad4 = AD.sin(ad1)\n\nprint('value: {}'.format(ad4.val))\nprint('derivative: {}'.format(ad4.der))","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00012-fbe9cd78-8d72-49a1-b612-a11323565775"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Software Organization\nWhat will the directory structure look like?\n- Other than the setup related files, we will have a directory for each of the following:\n\n- `auto_diff_pkg/` - all source code \n    - `AutoDiff.py`\n    - `__init__.py`\n- `docs/` - documentation and usage examples \n    - `demo.ipynb`\n    - `milestone1.ipynb`\n    - `milestone2_progress.ipynb`\n    - `milestone2.ipynb`\n- `tests/` - test suites files \n    - `test_AutoDiff.py`\n- `.travis.yml`\n- `README.md`\n- `requirements.txt`\n \nWhat modules do you plan on including? What is their basic functionality?\n- Idealy we want our module to be as independent as possible and therefore we will try to rely only on one dependancy - `numpy` for elementary operations. \nThis will allow us to utilize the basic data structures and math operations we don't need to overload, including trigonometric functions, exponentiations, and square roots. \n- We will also rely on `matplotlib` for plots.\n- In order to run the test suite we also used pytest and codecov packages \n\nWhere will your test suite live? Will you use TravisCI? CodeCov?\n- Our tests will live under the `tests/` subdirectory of our project. \n- Test suite is integrated with Travis-ci so that all tests are running with every push done to the repository \n- Codecov is also integrated so that testing code coverage is evaluated with every new build done on travis. \n- The repository README file contains both testing and coverage status tags that are pulling information from Travis and CodeCov\n\nHow will you distribute your package?\n- We will use PyPI to distribute package. This is a relatively streamlined process that will allow the users to install the package directly with a pip command.\nThis will also validate our package with the current python standards for software distribution.\n- Our code will also be available on github for direct cloning for those who wish to extend our code or simply prefer to not use pip.\n\nHow will you package your software? Will you use a framework? If so, which one and why? If not, why not?\n- After reviewing several python framework, we have decided to only follow the requirements for PyPI packaging and distribution. \n- This is a well established and supported process.\nThere is no constraints on the testing or source code control tools we use. \nIs is simple enough so that we are sure this will not impact our project in terms of overhead. \n\nOther considerations:\n- We also took into account the fact that our team members are for the most part relatively new to python packaging and so we wanted to make sure we make choices that are appropriate.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00010-7a6e6c6a-d0cf-4008-8d4f-dce6592ab52f"}},{"cell_type":"markdown","source":"# Implementation\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"output_cleared":false,"cell_id":"00011-dcf8c171-28fc-4c21-b42c-c84880c33f35"}},{"cell_type":"markdown","source":"Core data structures:\n- Our core data structure is an `AutoDiff` object consisting of 2 float variables for storing values and derivatives. We recursively store values and derivatives for every elementary function.\n\nCore classes we will implement:\n- A class `AutoDiff` for elementary functions that takes the advantage of dunder methods to recursively calculate the intermediate values of the trace and stores the value and the derivative of an operation.\n- Within the same file in `AutoDiff.py` we have functions in the package that will perform the operations including trignometric functions, exponentiation, square root.\n- A class `AD_Trace` for the trace table and graph.\n\nMethods and name attributes our classes have:\n- Class`AutoDiff`: \n    - `__init__`\n    - `self.val` (value of an AutoDiff object that will be updated per elementary operation)\n    - `self.der` (derivative of an AutoDiff object that will be updated per elementary operation)\n    - `__neg__`\n    - operator overload for all elementary functions, including `__add__`, `__radd__`, `__sub__`, `__rsub__`, `__mul__`, `__rmul__`, `__truediv__`, `__rtruediv__`, `__pow__`, `__rpow__`\n    - `__str__`\n- Trignometric, exponentiation, square root functions that take an AutoDiff object as argument: `log`, `exp`, `sin`, `cos`, `tan`, `sqrt`\n- Class `AD_Trace` for ouputting graph and table\n    - `display_graph`\n    - `print_table`\n\nExternal dependencies we rely on:\n- `numpy` for evaluating the elementary operations.\n- `matplotlib` for potential graphing needs.\n\nHow will you deal with elementary functions like sin, sqrt, log, and exp (and all the others)?\n- Arithmatic operations such as `add`, `sub`, `mul`, `truediv`, `pow` are implemented as dunder methods in `AutoDiff` class\n- Elementary functions like like sin, sqrt, log, and exp will be done outside the class `AutoDiff` but still within our package module.\nThis is due to the fact that these math functions are not defined in Python out of the box (they need to be pulled from Numpy, Math etc.), \nand we wanted to make sure we utilize Python's built in order of operations so we can avoid having to parse any function strings. \nThese functions will operate on both `AutoDiff` objects as well as on other types such as int, float etc. \n\nWhat aspects have you not implemented yet? What else do you plan on implementing?\n- We have implemented the forward mode that calculates the derivatives for elementary functions. The future work includes implementing the `AD_Trace` class that outputs the graph and trace tables (`display_graph()` and `print_table()`). We will also be adding more methods in `AutoDiff` class such as `__repr__` and comparators (`__eq__`, `__lt__`, `__gt__`)\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"output_cleared":false,"cell_id":"00012-40de5ea2-2534-4e17-8411-1484d7761a77"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00013-270e1162-e860-40a4-9ee4-f91091233b1f","output_cleared":false,"source_hash":"44b455af","execution_millis":0,"execution_start":1605814681733},"source":"import numpy as np\n\nclass AutoDiff():\n\n    def __init__(self, value, deriv=1.0):\n        self.val = value\n        self.der = deriv\n\n    def __neg__(self):\n        pass\n\n    def __add__(self, other):\n        pass\n        \n    def __radd__(self, other): \n        pass\n          \n    def __mul__(self, other):\n        pass\n\n    def __rmul__(self, other):\n        pass\n    \n    def __sub__(self, other):\n        pass\n    \n    def __rsub__(self, other): \n        pass\n    \n    def __truediv__(self, other): \n        pass\n    \n    def __rtruediv__(self, other): \n        pass\n    \n    def __pow__(self, other):\n        pass\n        \n    def __rpow__(self, other):\n        pass\n\n#External elementary functions:\n\ndef log(x):\n    pass\n\ndef exp(x):\n    pass\n\ndef sin(x):\n    pass\n\ndef cos(x):\n    pass\n\ndef tan(x):\n    pass\n    \ndef sqrt(x):\n    pass\n","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"output_cleared":true,"source_hash":null,"execution_millis":1,"cell_id":"00013-24e42f3d-69db-4de8-8044-e4b62558d58d","execution_start":1605733881825},"source":"class AD_Trace:\n\n    def __init__(func_list):\n        self.status = \"incomplete\"\n        self.func_list = func_list  # list of functions that are AutoDiff objects\n\n    def display_graph():\n        #print out the graph representation of our function\n        if self.status == \"incomplete\":\n            raise Exception(\"Cannot print partial graph, call forward_mode first to complete calculation\")\n        else:\n            # Show graph \n    \n    def print_table()\n        # print the completed formatted table \n        # store elements of table in the elementary function steps\n        if self.status == \"incomplete\":\n            raise Exception(\"Cannot print partial table, call forward_mode first to complete calculation\")\n        else:\n            for element in self.elements:\n                print(element)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Future Features\nFrom our current implementation state, there are aspects about the software we have written that will need changing for the final iteration. These are the following changes we will be making:\n\n#### Extension Implementation: Backpropagation\nAs detailed in our previous milestone updates, we will be implementing backpropagation. Backpropagation is a useful method in neural networks to compute derivatives of the loss with respect to the weights ($\\frac{\\partial Loss}{\\partial W}$) in order to use it for gradient descent to find the optimal weights for a layer. \nFor a single layer, $\\frac{\\partial Loss}{\\partial W} = \\frac{\\partial Preactivation}{\\partial W} \\cdot \\frac{\\partial Output}{\\partial Preactivation} \\cdot \\frac{\\partial Loss}{\\partial Output}$.\n\nIt involves intensive computation of differentiation, and our AD class could be useful in this context. AutoDiff is great for calculating partial derivatives, so the only changes we will need to make is a class for performing backpropagation which relies on the AutoDiff class.\n\n#### Installation via PyPI:\nWe will be implementing the requirements detailed here: https://packaging.python.org/tutorials/packaging-projects/ \nin order to allow an easier way to import the package while managing the dependencies. \nUsers will be able to install the package using a pip installation command and then simply \nuse the import command in their code. This will require a lot of code restructuring, but should not alter the functionality of our code, other than making installation much easier.\n\n#### AutoDiff Class Extensions\nIn order to make the AutoDiff class more useful overall, we plan to implement a few more dunder methods into the main class. As of now, our plan is to add methods for:\n- __repr__\n- comparators __eq__,__lt__,__gt__, etc.\n\n#### Additional functions:\nWe would like to include additional functions that were not yet implemented such as sinh, cosh, tanh. \n\n#### Supporting Graph representation \nWe would like like to be able to present the graph of operations for forward mode. \n\n\n#### Supporting Display of Evaluation Table \n\n#### Implementation of Root Finder \nWe can, if time allows, add this as a feature of our module, similar to what was demonstrated in our demo notebook. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00016-f366ecdf-a08d-4224-9760-5d5f9d0ee236"}},{"cell_type":"markdown","source":"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00017-5acb2bcd-1d21-48fe-814f-6294045ac5b3"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"cca98a6a-82e2-45d7-8fa5-00d0fd6eb7fe","deepnote_execution_queue":[]}}