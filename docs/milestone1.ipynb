{"cells":[{"cell_type":"markdown","source":"## Introduction\n\nDifferentiation is important to computation, optimization and engineering, showing up everywhere from neural networks to physics equations and any field that requires the calculation of rate of change, extrema, and zeros of a function. Automatic differentiation (autodiff) allows for the automatic computation of precise derivatives of a values by applying the chain rule to  a sequence of elementary arithmetic operations and functions repeatedly. \n\n### Automatic differentiation differs from the finite difference method and symbolic method of differentiation. \n\n**Symbolic differtiation** find the derivative of a given forumula, gives a new forumula as an output and plugs in a value to find the derivative at that value. \n\nFor example to find the derivative of f where\n$$f\\left(x\\right) = x^3 + 3.$$\n\nWe get \n\n$$\\dfrac{d}{dx} f\\left(x\\right)= \\dfrac{d}{dx}x^3 + \\dfrac{d}{dx}3 .$$\n\nIf we combine derivative constant rule and power rule we get \n\n$$\\dfrac{d}{dx} f\\left(x\\right)= 3x^2 + 0 =  3x^2.$$\n\nThis allows calculation at machine precision and provides a solution to a class of problems, not just a single problems. However, this can lead to inefficent code and can be costly to evalute. \n\n**Finite difference method** estimates a derivative by computing the slope of a secant line through the points (x, f(x)) and (x + h, f(x + h)), choosing a small number for h. The slope of this secant line approaches the slope of the tangent line as h approaches zero.\n<img src=\"/home/jovyan/work/image-20201018-153111.png\" width=\"400\">\n\nTherefore the derivative of f at x is:\n$$ f'(x) = \\lim_{h \\to 0}\\dfrac{f\\left(x+h\\right) - f\\left(x\\right)}{h}$$\n\nThis aproach is quick and easy but suffers from accuracy and precision due to truncation and rounding errors. \n\n**Automatic Differentiation** is more precise than the finite differences method and more efficient than symbolic differentiation. It allows for the computation of the derivative to machine precision without forming the formula for the derivative by using the chain rule to decompose derivatives. \n\n $$y = f\\left(g\\left(h\\left(x\\right)\\right)\\right) = f\\left(g\\left(h\\left(w_{0}\\right)\\right)\\right) = f\\left(g\\left(w_{1}\\right)\\right) = f\\left(w_{2})\\right) = w_{3}$$\n \n $$w_{0}= x$$\n\n $$w_{1}= h\\left(w_{0}\\right)$$\n\n $$w_{2}= g\\left(w_{1}\\right)$$\n \n $$w_{3}= f\\left(w_{2}\\right)=y$$ \n \n The Chain rule gives\n \n $\n  \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial w_{2}}\\frac{\\partial w_{2}}{\\partial w_{1}}\\frac{\\partial w_{1}}{\\partial x}=\\frac{\\partial f(w_{2})}{\\partial w_{2}}\\frac{\\partial g(w_{1})}{\\partial w_{1}}\\frac{\\partial h(w_{0})}{\\partial x}.$","metadata":{"tags":[],"cell_id":"00000-3f4964f9-44b6-4e39-b4e8-aad1cf391ad9","output_cleared":false}},{"cell_type":"markdown","source":"## Background\n### Automatic Differntiation: the forward mode\n#### Review of the Chain Rule\nThe chain rule allows for computing the derivative of a composite funtion. If f and g are both differentiable then \nthe chain rule gives the derivative of $f(g(x))$ in terms of the derivatives of f and g, $$f'(g(x))g'(x)$$.\n\nThe derivative is $$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial g}\\dfrac{\\partial g}{\\partial x}$$\n\n##### Example: $f\\left(g\\left(x\\right)\\right) = \\ln\\left(x\\right)^7$\n$$\\dfrac{\\partial f}{\\partial g} = 7\\left(g\\right)^6, \\quad \\dfrac{\\partial g}{\\partial x} = \\dfrac{1}{x},\\quad \\Rightarrow \\quad \\dfrac{\\partial f}{\\partial x} = 7\\ln\\left(x\\right)^6*\\dfrac{1}{x}.$$\n\n#### Elementary Functions\nComplex functions can be broken down into simpler paired functions which can be applied to the chain rule. These elementary operations include the arithmetic operations (addition, subtraction, multiplication and division) and exponential and trigonometric functions whose derivatives we know. We can combine these elementary functions to make more complex functions and find the derivatives of these more complex functions with the chain rule. \n\n#### The Gradient\nWe can find the derivative of a function with multiple inputs by applying the chain rule. The derivative of $f(x) = g(u(x), v(x))$ is \n\n$  \\frac{\\partial f}{\\partial x} = \\frac{\\partial g}{\\partial u}\\frac{\\partial u}{\\partial x} + \\frac{\\partial g}{\\partial v}\\frac{\\partial v}{\\partial x}.$\n\nWe can write this as \n\n$  \\nabla_{x}g = \\sum_{i=1}^{n}{\\frac{\\partial g}{\\partial y_{i}}\\nabla y_{i}\\left(x\\right)}.$\n\nWith this formula we can find the partial derivatives for each input. \n\n#### Computational Trace\nWe can compute the derivative of elementary functions and combine them using the chain rule to find the derivative of more complex functions. \n\nConsider the following function $$ f\\left(x,y\\right) = \\exp\\left(-\\left(\\sin\\left(x\\right) - \\cos\\left(y\\right)\\right)^{2}\\right)$$  We'd like to evalute $f$ at the point $x= \\left(\\dfrac{\\pi}{2}, \\dfrac{\\pi}{3}\\right)$.\n\n\n| Trace |  Eleme Operation  |  Numerical Value  |  Elem Deriv  |  value in respect to x  |  value in respect to y  |\n| :------: | :----------------------: | :------------------------------: | :------: | :------: | :------: |\n| $x_{1}$ | x | $\\pi/2$ | x1. | 1 | 0 |\n| $x_{2}$ | y | $\\pi/3$ | x2. | 0 | 1 |\n| $v_{1}$ | $\\sin(x1)$ | 1 | $\\cos(x1)*\\dot{x}1$ | 0 | 0 |\n| $v_{2}$ | cos(x2) | 0.5 | $\\sin(x2)*\\dot{x}_{2}$ | 0 | $\\sqrt{3}/2$ |\n| ${v}_{3}$ | $v_{1} - v_{2}$ | 0.5 |  $\\dot{v}_{1} - \\dot{v}_{2}$ | 0 | $\\sqrt{3}/2$  |\n| ${v}_{4}$ | ${v}_{3}^2$ | 1/4 | $2{v}_{3}  \\dot{v}_{3}$ |0  |$\\sqrt{3}/2$   | \n| ${v}_{5}$ | $-{v}_{4}$ | -1/4 | $-\\dot{v}_{4}$ | 0 |- $\\sqrt{3}/2$ | \n| ${v}_{6}$| $exp({v}_{5})$ | $exp(-1/4)$ | $exp({v}_{5})*\\dot{v}_{5}.$  | 0 | $\\exp(-1/4)$ * $(-\\sqrt{3}/2)$  | \n\n#### The Forward mode\nWe work from the inside out, starting with what we want to evaluate \n$x= \\left(\\dfrac{\\pi}{2}, \\dfrac{\\pi}{3}\\right)$, then build out the actual function at each step subsituting the derivative of the inner functions  in the chain rule. \n The function f(x,y) is composed of elementary functions which we know the derivative of, we compute the derivative of the elementary functions then \n use the chain rule to build up to the larger function.\n\n\n","metadata":{"tags":[],"cell_id":"00001-e497f4cd-69fc-4e5c-9a04-b34191fe72c1","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-df9eb492-a32d-47d7-abbd-6f1c7a21e35b","output_cleared":false,"source_hash":"b623e53d","execution_start":1603401670954,"execution_millis":7},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00001-c36d2b51-741f-4f24-864a-6baa966b6b54","output_cleared":false}},{"cell_type":"markdown","source":"# How to Use DeriveMeCrazy Auto_diff:\n## Importing: \n\n### Direct import:\nUsers will be able to clone the project off of github and run it locally.\nOnce they have the code in their local env, they will be able to import the different classes implemented. \n\n\n## Installation via PyPI:\nWe will be implementing the requirements detailed here: https://packaging.python.org/tutorials/packaging-projects/\nin order to allow an easier way to import the package while managing the dependencies. \nUsers will be able to install the package using a pip installation command and then simply \nuse the import command in their code. \n","metadata":{"tags":[],"cell_id":"00000-31653442-a14a-4617-8633-071f370ee3e6","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-3b1dbfc3-d5bc-4e7a-8ecb-4bb2b912b5d7","output_cleared":false,"source_hash":"6f17a8cd","execution_start":1603401670993,"execution_millis":3056},"source":"pip install dmc_auto_diff","execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement dmc_auto_diff (from versions: none)\u001b[0m\n\u001b[31mERROR: No matching distribution found for dmc_auto_diff\u001b[0m\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n## Interacting: \n### Command line interface: \nSupport will be available for manually inputting a function for which the AD evaluation table for forward mode is calculated.\nThis will allow the user to type in their required function, and then select their desired output (i.e. evaluation table in full, final values only etc)\nFor example: ","metadata":{"tags":[],"cell_id":"00001-53adc0ce-8326-49d9-b964-778c7c48ac89","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-1b4a86c4-d59a-4485-ae14-a81f8cd4ec59","output_cleared":false,"source_hash":"743f08b2","execution_start":1603401674062,"execution_millis":275},"source":"import auto_diff as ad \n\nauto_diff.cli()","execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'auto_diff'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-519f85cc3c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mauto_diff\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mauto_diff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'auto_diff'"]}]},{"cell_type":"markdown","source":"### AD classes:\nUsers will also be able to import specific components and incorporate them in their code.\nFor instance importing just the function parsing component or just the derivative calculation function.\nExamples:\n","metadata":{"tags":[],"cell_id":"00002-a90b5818-aef3-471f-b865-0e52283d3736","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-5c45e46b-a87b-4bfc-8f12-04a5a5bf9e6f","output_cleared":false,"source_hash":"abe7fe17"},"source":"import auto_diff as ad \n\nvar_array(x,y,z)\nmy_func = string_to_function(my_string, var_array)\n\neval_point((1,2,3))\nderivative_res = diff_calc(my_func, eval_point)\n\nprint(diff_table(my_func, eval_point))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Software Organization\nWhat will the directory structure look like?\n- Other than the setup related files, we will have a directory for each of the following:\n\n- autodiff - all source code \n- docs - documentation and usage examples \n- tests - test suites files \n \nWhat modules do you plan on including? What is their basic functionality?\n- Idealy we want our module to be as independent as possible and therefore we will try to rely only on one dependancy - numpy\nThis will allow us to utilize the basic data structures and math operations we don't need to overload. \n\nWhere will your test suite live? Will you use TravisCI? CodeCov?\n- Our tests will live under the tests/ subdirectory of our project. \nWe will use TravisCI to create our test suites and also use CodeCov for test coverage monitoring.\n\nHow will you distribute your package?\n- We will use PyPI to distribute package. This is a relatively streamlined process that will allow the users to install the package directly with a pip command.\nThis will also validate our package with the current python standards for software distribution.\n- Our code will also be available on github for direct cloning for those who wish to extend our code or simply prefer to not use pip.\n\nHow will you package your software? Will you use a framework? If so, which one and why? If not, why not?\n- After reviewing several python framework, we have decided to only follow the requirements for PyPI packaging and distribution. \nThis is a well established and supported process.\nThere is no constraints on the testing or source code control tools we use. \nIs is simple enough so that we are sure this will not impact our project in terms of overhead. \n\nOther considerations?\n- We also took into account the fact that our team members are for the most part relatively new to python packaging and so we wanted to make sure we make choices that are appropriate .","metadata":{"tags":[],"cell_id":"00010-eb5a3b60-7541-47dc-9d18-373728bdbca8"}},{"cell_type":"markdown","source":"# Implementation\n","metadata":{"tags":[],"cell_id":"00001-a1398856-ec26-4803-a6d4-ec44d26440cd","output_cleared":false}},{"cell_type":"markdown","source":"What are the core data structures?\n- an array will hold the operations done for how the actual function would evaluate\n- an array of tuples will hold the output of the functions and their derivatives for each function\n\nWhat classes will you implement?\n- A class (or just a function in the package) to parse a function (given as a string) to it's elementary functions\n- A class that contains all trace elements that takes a list of operations (which make up the function) and associated values for each function, as well as the dimensionality of the function\n- A class for the intermediate values of the trace (given previous element value/derivatives and function to perform at that step, contains a function for evaluating that step and returns next value and derivative)\n\nWhat method and name attributes will your classes have?\n- The class that contains all trace elements: __init__, self.func_list(list of strings), self.val_func_list(list of floats), forward_mode(returns func val and derivative)\n- intermediate value class: __init__, self.func(string), self.value, self.deriv, operator overload for all elementary functions, get_val_deriv(returns intermediate func val and derivative(s))\n\nWhat external dependencies will you rely on?\n- numpy should be the only thing needed to evaluate the elementary operations\n- matplotlib for potential graphing needs\n\nHow will you deal with elementary functions like sin, sqrt, log, and exp (and all the others)?\n- All elementary functions will be overridden in the class, but functions like the ones above will be done within the class method to ensure the derivative is also calculated\n","metadata":{"tags":[],"cell_id":"00007-18041ef6-7824-4437-ada4-a1043b83ee8a","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-38d2ab27-63dc-42e3-a1dc-f4238542a565","output_cleared":false,"source_hash":"bedbed93","execution_millis":41},"source":"class function_parser:\n\n    def __init__(func_string):\n        self.func_string = func_string\n        #initializing the current representation of our function\n        #in terms of our already evaluated nodes \n        self.curr_func_string = \"\"\n        \n    def parse_next():\n        #return the next ad_trace_elem object to create a node for\n        #rewrites the curr_func_string to the new representations \n\n    def parse_func():\n        # 2*x + 1\n        # ['*2','+1']\n        # parses the function string into its elementary functions\n\n        return func_list\n\n#basic component enum\ndef class op_type(Enum):\n    Symbol = 1\n    Value = 2\n\ndef class operand:\n    def __init__(op_type):\n        self.op_type = op_type\n    def set_value(op_val):\n        self.op_val = op_val\n\nclass ad_trace_elem:\n\n    def __init__(parent_ad_trace,operation,val_func, op_list):\n        self.parent_ad_trace = parent_ad_trace\n        self.derivs = [] #value of our current node partial derivatives that are calculated \n        self.operation = operation # operation to be used for updating the trace (as a string, e.g 'sin')\n        self.val_func = evaluate() # value associated with the function (e.g if func is 'mult', then val_func is the multiplier)\n        #we can have one or two depending on the type\n        self.operands = op_list\n        self.curr_operand = operand(1)\n\n    def get_val_deriv():\n        return (self.value, self.derivs)\n    \n    def __str__():\n        #Go over element attributes and return a string of this table row \n        return element_string\n\n    def diff(operand_to_diff_by):\n        #calculate the partial derivative of this node by the requested operand\n        #and store the value in our element\n    \n    def evaluate():\n        #uses previous node's values and the overloaded operation function \n        #To obtain and calculate the value of the current node \n        #We will have a sort of Switch statement here for operation selection \n        for op in op_list:\n            derivs.append(diff(op))\n\n    ## operator overloads for all elementary functions\n\n\n\nclass ad_trace:\n\n    def __init__(func_list,val_func,input_vars, function_string, variable_list):\n        self.func_list = func_list # elementary functions in order needed to evaluate the trace\n        self.val_func_list = val_func_list # value associated with each elementary function\n        #If I understand correctly we can get this number from the length of the list of variable_list\n        self.input_vars = input_vars # determines how many derivatives are needed at each trace step\n        self.status = incomplete\n        self.function_string = function_string\n        self.variable_list = variable_list # the list of operands of type Symbol in our function \n        self.elements_dict = {} #initialize the dictionary holding all our operands and symbols \n        self.function_parser = function_parser(function_string)\n\n    def forward_mode(point):\n        # Ensure the dimensionality of point is equal to the number of input variables\n        # Evaluate the trace at this point\n        # returns value and derivative at that point wrt each dimension\n        while(self.status == \"incomplete\"):\n            #parse the next operation \n\n            #generate a new operand node\n            next_node = self.function_parser.parse_next()\n            if function_parser.curr_func_string == \"\":\n                self.status = complete\n                berak;\n            else: \n                next_node.evaluate()\n        pass\n\n    def display_graph():\n        #print out the graph representation of our function\n        if self.status == \"incomplete\":\n            raise Exception(\"Cannot print partial graph, call forward_mode first to complete calculation\")\n        else:\n            #Show graph \n    \n    def print_table()\n        #print the completed formatted table \n        if self.status == \"incomplete\":\n            raise Exception(\"Cannot print partial table, call forward_mode first to complete calculation\")\n        else:\n            for element in self.elements:\n                print(element)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Selected extension: Implementing AD in Backpropagation\n\nBackpropagation is a useful method in neural networks to compute derivatives of the loss with respect to the weights ($\\frac{\\partial Loss}{\\partial W}$) in order to use it for gradient descent to find the optimal weights for a layer. \nFor a single layer, $\\frac{\\partial Loss}{\\partial W} = \\frac{\\partial Preactivation}{\\partial W} \\cdot \\frac{\\partial Output}{\\partial Preactivation} \\cdot \\frac{\\partial Loss}{\\partial Output}$.\n\nIt involves intensive computation of differentiation, and our AD class could be useful in this context.","metadata":{"tags":[],"cell_id":"00010-c991fc52-c720-4217-a826-b653963cb4e5","output_cleared":false}},{"cell_type":"markdown","source":"## Milestone 1 Feedback:\n\n\n","metadata":{"tags":[],"cell_id":"00015-0f098029-0299-4c90-8c8e-0d96bd8fae7e"}},{"cell_type":"markdown","source":"#### Background section \n- add a computational graph example:\n##### Computational Graph\n\n<img src=\"/home/jovyan/work/forward graph.png/home/jovyan/work/forwardgraph.png\" width=\"400\">\n","metadata":{"tags":[],"cell_id":"00016-e2a251c1-3ae6-4be4-92c0-2f0123ed35d1"}},{"cell_type":"markdown","source":"#### Implementation section \n- consider overloading operator dunders for sake of efficiency: \n","metadata":{"tags":[],"cell_id":"00017-f10bc819-2333-4569-928c-a2366e8fa711"}},{"cell_type":"markdown","source":"Overloading operator dunders is very useful when dealing with mixed types. In this project we know our variables values will belong to R (real numbers) which include several different types of numbers (int, float etc)\nSo this will allow us to make sure we support all types efficiently.\nAn even greater value can be found in overloading these operator in this specific project of automatic differentiation.\nAs mentioned in the Implementation section above we have described our high level consept of creating an Operand class to model our automatic differentiation vareables, so overloading operator dunders will allow us to use basic arithmetics that will work on Operand objects as well\nand of course also support mixed type arithmetic. \n","metadata":{"tags":[],"cell_id":"00018-b1bfaeb8-5a3b-4b2b-a0e8-0112d8088e92"}},{"cell_type":"markdown","source":"- import special function like cosine/sine from numpy:\n","metadata":{"tags":[],"cell_id":"00018-d3e5887f-8517-40fb-8cea-3c862bdc5598"}},{"cell_type":"markdown","source":"As mentioned above, we will attempt to only rely on a single dependency import which will be the numpy library.\nWe will make use of this library to obtain support of functions that include non linear components such as sine, cosine, e, pi etc. \n ","metadata":{"tags":[],"cell_id":"00018-cfa1f8f4-bee2-48ff-902f-44461560ab07"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"26e069f9-0e3e-4169-903b-6f693af3d519","deepnote_execution_queue":[]}}