{"cells":[{"cell_type":"markdown","source":"## Introduction\n\nDifferentiation is important to computation, optimization and engineering, showing up everywhere from neural networks to physics equations and any field that requires the calculation of rate of change, extrema, and zeros of a function. Automatic differentiation (autodiff) allows for the automatic computation of precise derivatives of a values by applying the chain rule to  a sequence of elementary arithmetic operations and functions repeatedly. \n\n### Automatic differentiation differs from the finite difference method and symbolic method of differentiation. \n\n**Symbolic differtiation** find the derivative of a given forumula, gives a new forumula as an output and plugs in a value to find the derivative at that value. \n\nFor example to find the derivative of f where\n$$f\\left(x\\right) = x^3 + 3.$$\n\nWe get \n\n$$\\dfrac{d}{dx} f\\left(x\\right)= \\dfrac{d}{dx}x^3 + \\dfrac{d}{dx}3 .$$\n\nIf we combine derivative constant rule and power rule we get \n\n$$\\dfrac{d}{dx} f\\left(x\\right)= 3x^2 + 0 =  3x^2.$$\n\nThis allows calculation at machine precision and provides a solution to a class of problems, not just a single problems. However, this can lead to inefficent code and can be costly to evalute. \n\n**Finite difference method** estimates a derivative by computing the slope of a secant line through the points (x, f(x)) and (x + h, f(x + h)), choosing a small number for h. The slope of this secant line approaches the slope of the tangent line as h approaches zero.\n<img src=\"/home/jovyan/work/image-20201018-153111.png\" width=\"400\">\n\nTherefore the derivative of f at x is:\n$$ f'(x) = \\lim_{h \\to 0}\\dfrac{f\\left(x+h\\right) - f\\left(x\\right)}{h}$$\n\nThis aproach is quick and easy but suffers from accuracy and precision due to truncation and rounding errors. \n\n**Automatic Differentiation** is more precise than the finite differences method and more efficient than symbolic differentiation. It allows for the computation of the derivative to machine precision without forming the formula for the derivative by using the chain rule to decompose derivatives. \n\n $$y = f\\left(g\\left(h\\left(x\\right)\\right)\\right) = f\\left(g\\left(h\\left(w_{0}\\right)\\right)\\right) = f\\left(g\\left(w_{1}\\right)\\right) = f\\left(w_{2})\\right) = w_{3}$$\n \n $$w_{0}= x$$\n\n $$w_{1}= h\\left(w_{0}\\right)$$\n\n $$w_{2}= g\\left(w_{1}\\right)$$\n \n $$w_{3}= f\\left(w_{2}\\right)=y$$ \n \n The Chain rule gives\n \n $\n  \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial w_{2}}\\frac{\\partial w_{2}}{\\partial w_{1}}\\frac{\\partial w_{1}}{\\partial x}=\\frac{\\partial f(w_{2})}{\\partial w_{2}}\\frac{\\partial g(w_{1})}{\\partial w_{1}}\\frac{\\partial h(w_{0})}{\\partial x}.$","metadata":{"cell_id":"00000-0b07f0f1-e09b-4f6a-b720-52fee5c8761a","output_cleared":false,"tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Background\n### Automatic Differntiation: the forward mode\n#### Review of the Chain Rule\nThe chain rule allows for computing the derivative of a composite funtion. If f and g are both differentiable then \nthe chain rule gives the derivative of $f(g(x))$ in terms of the derivatives of f and g, $$f'(g(x))g'(x)$$.\n\nThe derivative is $$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial g}\\dfrac{\\partial g}{\\partial x}$$\n\n##### Example: $f\\left(g\\left(x\\right)\\right) = \\ln\\left(x\\right)^7$\n$$\\dfrac{\\partial f}{\\partial g} = 7\\left(g\\right)^6, \\quad \\dfrac{\\partial g}{\\partial x} = \\dfrac{1}{x},\\quad \\Rightarrow \\quad \\dfrac{\\partial f}{\\partial x} = 7\\ln\\left(x\\right)^6*\\dfrac{1}{x}.$$\n\n#### Elementary Functions\nComplex functions can be broken down into simpler paired functions which can be applied to the chain rule. These elementary operations include the arithmetic operations (addition, subtraction, multiplication and division) and exponential and trigonometric functions whose derivatives we know. We can combine these elementary functions to make more complex functions and find the derivatives of these more complex functions with the chain rule. \n\n#### The Gradient\nWe can find the derivative of a function with multiple inputs by applying the chain rule. The derivative of $f(x) = g(u(x), v(x))$ is \n\n$  \\frac{\\partial f}{\\partial x} = \\frac{\\partial g}{\\partial u}\\frac{\\partial u}{\\partial x} + \\frac{\\partial g}{\\partial v}\\frac{\\partial v}{\\partial x}.$\n\nWe can write this as \n\n$  \\nabla_{x}g = \\sum_{i=1}^{n}{\\frac{\\partial g}{\\partial y_{i}}\\nabla y_{i}\\left(x\\right)}.$\n\nWith this formula we can find the partial derivatives for each input. \n\n#### Computational Trace\nWe can compute the derivative of elementary functions and combine them using the chain rule to find the derivative of more complex functions. \n\nConsider the following function $$ f\\left(x,y\\right) = \\exp\\left(-\\left(\\sin\\left(x\\right) - \\cos\\left(y\\right)\\right)^{2}\\right)$$  We'd like to evalute $f$ at the point $x= \\left(\\dfrac{\\pi}{2}, \\dfrac{\\pi}{3}\\right)$.\n\n\n| Trace |  Eleme Operation  |  Numerical Value  |  Elem Deriv  |  value in respect to x  |  value in respect to y  |\n| :------: | :----------------------: | :------------------------------: | :------: | :------: | :------: |\n| $x_{1}$ | x | $\\pi/2$ | x1. | 1 | 0 |\n| $x_{2}$ | y | $\\pi/3$ | x2. | 0 | 1 |\n| $v_{1}$ | $\\sin(x1)$ | 1 | $\\cos(x1)*\\dot{x}1$ | 0 | 0 |\n| $v_{2}$ | cos(x2) | 0.5 | $\\sin(x2)*\\dot{x}_{2}$ | 0 | $\\sqrt{3}/2$ |\n| ${v}_{3}$ | $v_{1} - v_{2}$ | 0.5 |  $\\dot{v}_{1} - \\dot{v}_{2}$ | 0 | $\\sqrt{3}/2$  |\n| ${v}_{4}$ | ${v}_{3}^2$ | 1/4 | $2{v}_{3}  \\dot{v}_{3}$ |0  |$\\sqrt{3}/2$   | \n| ${v}_{5}$ | $-{v}_{4}$ | -1/4 | $-\\dot{v}_{4}$ | 0 |- $\\sqrt{3}/2$ | \n| ${v}_{6}$| $exp({v}_{5})$ | $exp(-1/4)$ | $exp({v}_{5})*\\dot{v}_{5}.$  | 0 | $\\exp(-1/4)$ * $(-\\sqrt{3}/2)$  | \n\n#### The Forward mode\nWe work from the inside out, starting with what we want to evaluate \n$x= \\left(\\dfrac{\\pi}{2}, \\dfrac{\\pi}{3}\\right)$, then build out the actual function at each step subsituting the derivative of the inner functions  in the chain rule. \n The function f(x,y) is composed of elementary functions which we know the derivative of, we compute the derivative of the elementary functions then \n use the chain rule to build up to the larger function.\n\n\n","metadata":{"cell_id":"00001-8af783ff-96c3-4850-8dcd-0e248ceaaef3","output_cleared":false,"tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# How to Use DeriveMeCrazy Auto_diff:\n## Installing\nThe package is hosted on PyPI and can be installed with a simple pip command:\n\n\n\n","metadata":{"cell_id":"00002-eac211b2-4026-452b-81bc-1f4746078888","output_cleared":false,"tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00003-234be9f6-1383-43ce-b973-f7b6db11307c","execution_millis":3056,"execution_start":1603401670993,"output_cleared":true,"source_hash":null,"tags":[],"deepnote_cell_type":"code"},"source":"pip install auto_diff_pkg","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Direct import:\nUsers will be able to clone the project off of github and run it locally.\nOnce they have the code in their local env, they will be able to install with pip and import the different classes implemented. ","metadata":{"cell_id":"00004-d1b32a71-47c3-4829-abb0-3c107f54ea55","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00005-3f9eeeac-08d0-42a7-844a-ec2876bd5c4a","deepnote_cell_type":"code"},"source":"pip install .","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Demo\n\nWe include a basic demo below, including how to import, create an object, and use `AutoDiff` for calculating derivatives in an arithmetic operation and a trignometric operation. A more detailed demo for more types of operations is included in `docs/demo.ipynb`.","metadata":{"cell_id":"00006-e45c8add-a4ec-486b-9774-05be5730842c","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"#### Import","metadata":{"cell_id":"00007-09babd03-353c-455b-af5f-149756c268d5","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00008-4ec5471a-41fa-41bd-abcc-5bf3c95b0869","tags":[],"deepnote_cell_type":"code"},"source":"import auto_diff_pkg.AutoDiff as AD","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating an `AutoDiff` object","metadata":{"cell_id":"00009-25edd0dd-f251-4073-85e8-a5db0c953031","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00010-179cfe42-2ca2-4fb8-b076-9224d2a5148f","tags":[],"deepnote_cell_type":"code"},"source":"ad1 = AD.AutoDiff(5.0)\nad2 = AD.AutoDiff(3.0)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Arithmetic operations","metadata":{"cell_id":"00011-e62f71ae-c96d-4a6a-8220-23ea3a480c0f","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00012-6305ca4c-9bf9-4110-8ed5-8a8b749f3ec4","tags":[],"deepnote_cell_type":"code"},"source":"ad3 = ad1 * ad2\n\nprint('value: {}'.format(ad3.val))\nprint('derivative: {}'.format(ad3.der))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Trignometric operations","metadata":{"cell_id":"00013-faf00d00-a0f1-4f69-b1d9-7b9e92f97fea","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00014-c3c9fa59-235d-40a4-a420-72369de4259c","tags":[],"deepnote_cell_type":"code"},"source":"ad4 = AD.sin(ad1)\n\nprint('value: {}'.format(ad4.val))\nprint('derivative: {}'.format(ad4.der))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Software Organization\n- Other than the setup related files, we will have a directory for each of the following:\n\n- `auto_diff_pkg/` - all source code \n    - `__init__.py`\n    - `AutoDiff.py`\n    - `ReverseAutoDiff.py`\n- `docs/` - documentation and usage examples\n    - `demo.ipynb`\n    - `milestone1.ipynb`\n    - `milestone2_progress.ipynb`\n    - `milestone2.ipynb`\n    - `documentation.ipynb`\n- `tests/` - test suites files \n    - `test_AutoDiff.py`\n    - `test_ReverseAutoDiff.py`\n- `.travis.yml`\n- `README.md`\n- `setup.py`\n- `LICENSE`\n \nWhat modules do you plan on including? What is their basic functionality?\n- Ideally we want our module to be as independent as possible and therefore we will try to rely only on one dependancy - `numpy` for elementary operations. \nThis will allow us to utilize the basic data structures and math operations we don't need to overload, including trigonometric functions, exponentiations, and square roots. \n- In order to run the test suite we also used `pytest` and `codecov` packages \n\nWhere will your test suite live? Will you use TravisCI? CodeCov?\n- Our tests will live under the `tests/` subdirectory of our project. \n- Test suite is integrated with Travis-ci so that all tests are running with every push done to the repository \n- Codecov is also integrated so that testing code coverage is evaluated with every new build done on travis. \n- The repository README file contains both testing and coverage status tags that are pulling information from Travis and CodeCov\n\nHow will you distribute your package?\n- We will use Github to distribute our package. Users can clone the repository and is set up in such a way that will allow them to install the package directly with a pip command. This will also validate our package with the current python standards for software distribution.\n- Our code will also be available on github for direct cloning for those who wish to extend our code or simply prefer to not use pip.\n\nHow will you package your software? Will you use a framework? If so, which one and why? If not, why not?\n- After reviewing several python framework, we have decided to only follow the requirements for PyPI packaging and distribution. \n- This is a well established and supported process.\nThere is no constraints on the testing or source code control tools we use. \nIs is simple enough so that we are sure this will not impact our project in terms of overhead. \n\nOther considerations:\n- We also took into account the fact that our team members are for the most part relatively new to python packaging and so we wanted to make sure we make choices that are appropriate.","metadata":{"cell_id":"00015-a57bd8c0-9358-4ac3-b4ab-5840cd8b091d","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Implementation\n","metadata":{"cell_id":"00016-9c8d932f-699c-4ba5-8824-d3338405d508","output_cleared":false,"tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Core data structures:\n- Our core data structure is an `AutoDiff` object consisting of 2 float variables for storing values and derivatives. We recursively store values and derivatives for every elementary function. The class also takes two additional inputs, variables and position for handling multiple functions and variables.\n\nCore classes we will implement:\n- A class `AutoDiff` for elementary functions that takes the advantage of dunder methods to recursively calculate the intermediate values of the trace and stores the value and the derivative of an operation.\n- Within the same file in `AutoDiff.py` we have functions in the package that will perform the operations including trignometric functions, exponentiation, square root.\n- A class `ReverseADNode` for elementary functions that relate the reverse mode (see extension section below)\n- Within the same file in `ReverseAutoDiff.py` we have functions in the package that will perform the operations including trignometric functions, exponentiation, and square root for the reverse mode.\n\nMethods and name attributes our classes have:\n- Class`AutoDiff`: \n    - `__init__`\n        - `self.val` (value of an AutoDiff object that will be updated per elementary operation)\n        - `self.der` (derivative of an AutoDiff object that will be updated per elementary operation)\n        - `self.variables` (number of variables used in a multivariable function)\n        - `self.position` (position of this variable in a multivariable function)\n    - - `__str__`, `__neg__`, `__eq__`, and `__neq__`\n    - operator overload for all elementary functions, including `__add__`, `__radd__`, `__sub__`, `__rsub__`, `__mul__`, `__rmul__`, `__truediv__`, `__rtruediv__`, `__pow__`, `__rpow__`\n    - Trignometric, exponentiation, square root functions that take an AutoDiff object as argument: `log`, `exp`, `sin`, `cos`, `tan`, `sqrt`\n- Class `ReverseADNode`:\n    - `__init__`\n        - `self.value` (current value of the node)\n        - `self.children` (children of this reverse mode operations)\n        - `self.grad_value` (current value of the gradient)\n        - `self.op` (associated elementary operation)\n    - `grad` (for solving the gradient)\n    - operator overload for all elementary functions, including `__add__`, `__radd__`, `__sub__`, `__rsub__`, `__inv__`, `__mul__`, `__rmul__`, `__truediv__`, `__rtruediv__`, `__pow__`, `__rpow__`\n    - `graph` (for printing the trace related to that node)\n    - - Trignometric, exponentiation, square root functions that take an AutoDiff object as argument: `log`, `exp`, `sin`, `cos`, `tan`, `sqrt`, `arcsin`, `arccos`, `arctan`, `sinh`, `cosh`, `tanh`, \n- method `jacobian` (for evaluating the jacobian of a system of equations with multiple variables for both the forward and reverse mode)\n\nExternal dependencies we rely on:\n- `numpy` for evaluating the elementary operations.\n- `pytest` for evaluating our test suite.\n- `notebook` for enabling the interactive demo.","metadata":{"cell_id":"00017-3ebf9b62-2c49-4863-a027-f0a0802fe040","output_cleared":false,"tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Extension: Reverse Mode\nOur initial plan for the first two milestones was to implement backpropagation for our extenstion. However, as a team we decided that it would be more beneficial if the program instead implements the reverse mode. The reason for this is because backpropagation is a particular case of the reverse mode. By instead implementing the full reverse mode, this gives our users access to more flexibility and versatility. In comparison to the forward mode, where the final product is the Jacobian-vector product $Jp$, the final product of the reverse mode is $J^Tp$. By implementing the reverse mode, users of our package will be able to get the most efficient solution, regardless of the number of $m$ seed vectors and $n$ functions. When the user encounters a system where $n >> m$, they can use the forward mode implementation, and when they have a system where $m >> n$, they can use the reverse mode implementation.\n\nThe reverse mode starts by taking a forward pass through the the elementary functions, storing the partial derivatives along the way (without evaluation the chain rule). We then iterate from the end of the trace, multiplying the current partial derivative with the previous, i.e $v_N=\\frac{\\delta f}{\\delta v_N}$, $v_{N-1}=\\frac{\\delta f}{\\delta v_N}\\frac{\\delta v_N}{\\delta v_{N-1}}$, and so on. The when a partial derivative has multiple children in the trace, their sum is taken instead. The structure and implementation code for doing so is given above in conjunction with the forward mode specifics. \n\n$$u_i += \\frac{\\partial f}{\\partial u_j} \\frac{\\partial u_j}{\\partial u_i}$$\n$$\\frac{\\partial f}{\\partial u_i} = \\sum_{j a child of i}\\frac{\\partial f}{\\partial u_j} \\frac{\\partial u_j}{\\partial u_i}$$\n\nFor a more in-depth comparison of the performance between the two modes, please see `docs/Performance_comparison.ipynb`","metadata":{"cell_id":"00018-197ba05f-b793-4367-bddb-f7a151b10ff8","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Broader Impact and Inclusivity Statement\n\n### Broader Impact\n\nWe propose a method to perform differentiation conveniently and accurately. The automatic differentiation package is able to efficiently compute the derivatives of functions of any inputs, including integers, floats, single and multiple variables. Traditionally in finite differentiation, users need to select an epsilon value for the algorithm that calculates the difference of slope. The choice of epsilon will impact the accuracy of the derivative. Our package eliminates this process for users so that they can have an accurate calculation without worrying about the choice of epsilon.\n\nWhile automatic differentiation is proven to be powerful in calculating accurate derivatives, such function does not prevail in common machine learning packages. In neural networks and regression based models, gradient descent is widely used to find the optimal parameters. Automatic differentiation assists this process so that any differentiation, even when the algebraic form is hard to compute, can be done easily. This broadens the range of models one can choose from without concerning the complexity of their derivatives. If used responsibly, the benefit of a wider range of models and increasing accuracy can be broadcast to many fields including public health and medicine, where models are rather complicated.\n\n### Inclusivity\n\nWe strongly believe in the importance of inclusivity of our package. We worked to ensure that our package is accessible to all and includes documentation that is easy to understand regardless of cultural and language backgrounds. The creation of this package was conducted through teamwork where every member was respected and represented, and contributed to the outcome. The coding process was discussed among members of the team, and pull requests were reviewed by members of the team before merging.\n\nOur purpose of this package is to construct a better method of differentiation, and potentially to increase efficiency in machine learning. We discourage any illegal and unethical use of our package in projects that harm a particular group based on attributes including (but not limited to) age, culture, ethnicity, gender identity or expression, national origin, physical or mental difference, politics, race, religion, sex, sexual orientation, socio-economic status, and subculture.","metadata":{"cell_id":"00019-4a2efa44-4ba0-42ab-a3c6-c584cfccb302","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Future Changes\n\nThere are still a few things that would make this project more complete and higher quality. Those potential updates are listed below:\n- An added `__repr__` dunder method to each class to assist developers using this package.\n- Adding a graphical representation to the forward and reverse modes to make the elementary operations more transparent to the user.\n- Adding a way to print the evaluation table back to the user.\n- Including a root finder implementation that is inherent to the code, as this is one of the most useful applications for automatic differentiation.","metadata":{"cell_id":"00020-5c6fc915-d9d6-4865-a80e-467dcf7abd1d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00021-ef3040d0-4c11-4c1c-b865-cd174b8cbdcd","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00022-91fc5772-2f71-4385-a004-661f2d597485","tags":[],"deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00023-5321d698-c6ae-40cc-b27e-6b148133f779","deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":4,"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"44af4a74-48ab-4a87-a405-5bc1df059937","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}}}