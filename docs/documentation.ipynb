{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-65cfbae6-1fcb-4ba4-98fc-141176907b4a",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Differentiation is important to computation, optimization and engineering, showing up everywhere from neural networks to physics equations and any field that requires the calculation of rate of change, extrema, and zeros of a function. Automatic differentiation (autodiff) allows for the automatic computation of precise derivatives of a values by applying the chain rule to  a sequence of elementary arithmetic operations and functions repeatedly. \n",
    "\n",
    "### Automatic differentiation differs from the finite difference method and symbolic method of differentiation. \n",
    "\n",
    "**Symbolic differtiation** find the derivative of a given forumula, gives a new forumula as an output and plugs in a value to find the derivative at that value. \n",
    "\n",
    "For example to find the derivative of f where\n",
    "$$f\\left(x\\right) = x^3 + 3.$$\n",
    "\n",
    "We get \n",
    "\n",
    "$$\\dfrac{d}{dx} f\\left(x\\right)= \\dfrac{d}{dx}x^3 + \\dfrac{d}{dx}3 .$$\n",
    "\n",
    "If we combine derivative constant rule and power rule we get \n",
    "\n",
    "$$\\dfrac{d}{dx} f\\left(x\\right)= 3x^2 + 0 =  3x^2.$$\n",
    "\n",
    "This allows calculation at machine precision and provides a solution to a class of problems, not just a single problems. However, this can lead to inefficent code and can be costly to evalute. \n",
    "\n",
    "**Finite difference method** estimates a derivative by computing the slope of a secant line through the points (x, f(x)) and (x + h, f(x + h)), choosing a small number for h. The slope of this secant line approaches the slope of the tangent line as h approaches zero.\n",
    "<img src=\"/home/jovyan/work/image-20201018-153111.png\" width=\"400\">\n",
    "\n",
    "Therefore the derivative of f at x is:\n",
    "$$ f'(x) = \\lim_{h \\to 0}\\dfrac{f\\left(x+h\\right) - f\\left(x\\right)}{h}$$\n",
    "\n",
    "This aproach is quick and easy but suffers from accuracy and precision due to truncation and rounding errors. \n",
    "\n",
    "**Automatic Differentiation** is more precise than the finite differences method and more efficient than symbolic differentiation. It allows for the computation of the derivative to machine precision without forming the formula for the derivative by using the chain rule to decompose derivatives. \n",
    "\n",
    " $$y = f\\left(g\\left(h\\left(x\\right)\\right)\\right) = f\\left(g\\left(h\\left(w_{0}\\right)\\right)\\right) = f\\left(g\\left(w_{1}\\right)\\right) = f\\left(w_{2})\\right) = w_{3}$$\n",
    " \n",
    " $$w_{0}= x$$\n",
    "\n",
    " $$w_{1}= h\\left(w_{0}\\right)$$\n",
    "\n",
    " $$w_{2}= g\\left(w_{1}\\right)$$\n",
    " \n",
    " $$w_{3}= f\\left(w_{2}\\right)=y$$ \n",
    " \n",
    " The Chain rule gives\n",
    " \n",
    " $\n",
    "  \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial w_{2}}\\frac{\\partial w_{2}}{\\partial w_{1}}\\frac{\\partial w_{1}}{\\partial x}=\\frac{\\partial f(w_{2})}{\\partial w_{2}}\\frac{\\partial g(w_{1})}{\\partial w_{1}}\\frac{\\partial h(w_{0})}{\\partial x}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-ee1c0405-c4e6-4303-acd1-175db0655961",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Background\n",
    "### Automatic Differntiation: the forward mode\n",
    "#### Review of the Chain Rule\n",
    "The chain rule allows for computing the derivative of a composite funtion. If f and g are both differentiable then \n",
    "the chain rule gives the derivative of $f(g(x))$ in terms of the derivatives of f and g, $$f'(g(x))g'(x)$$.\n",
    "\n",
    "The derivative is $$\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial g}\\dfrac{\\partial g}{\\partial x}$$\n",
    "\n",
    "##### Example: $f\\left(g\\left(x\\right)\\right) = \\ln\\left(x\\right)^7$\n",
    "$$\\dfrac{\\partial f}{\\partial g} = 7\\left(g\\right)^6, \\quad \\dfrac{\\partial g}{\\partial x} = \\dfrac{1}{x},\\quad \\Rightarrow \\quad \\dfrac{\\partial f}{\\partial x} = 7\\ln\\left(x\\right)^6*\\dfrac{1}{x}.$$\n",
    "\n",
    "#### Elementary Functions\n",
    "Complex functions can be broken down into simpler paired functions which can be applied to the chain rule. These elementary operations include the arithmetic operations (addition, subtraction, multiplication and division) and exponential and trigonometric functions whose derivatives we know. We can combine these elementary functions to make more complex functions and find the derivatives of these more complex functions with the chain rule. \n",
    "\n",
    "#### The Gradient\n",
    "We can find the derivative of a function with multiple inputs by applying the chain rule. The derivative of $f(x) = g(u(x), v(x))$ is \n",
    "\n",
    "$  \\frac{\\partial f}{\\partial x} = \\frac{\\partial g}{\\partial u}\\frac{\\partial u}{\\partial x} + \\frac{\\partial g}{\\partial v}\\frac{\\partial v}{\\partial x}.$\n",
    "\n",
    "We can write this as \n",
    "\n",
    "$  \\nabla_{x}g = \\sum_{i=1}^{n}{\\frac{\\partial g}{\\partial y_{i}}\\nabla y_{i}\\left(x\\right)}.$\n",
    "\n",
    "With this formula we can find the partial derivatives for each input. \n",
    "\n",
    "#### Computational Trace\n",
    "We can compute the derivative of elementary functions and combine them using the chain rule to find the derivative of more complex functions. \n",
    "\n",
    "Consider the following function $$ f\\left(x,y\\right) = \\exp\\left(-\\left(\\sin\\left(x\\right) - \\cos\\left(y\\right)\\right)^{2}\\right)$$  We'd like to evalute $f$ at the point $x= \\left(\\dfrac{\\pi}{2}, \\dfrac{\\pi}{3}\\right)$.\n",
    "\n",
    "\n",
    "| Trace |  Eleme Operation  |  Numerical Value  |  Elem Deriv  |  value in respect to x  |  value in respect to y  |\n",
    "| :------: | :----------------------: | :------------------------------: | :------: | :------: | :------: |\n",
    "| $x_{1}$ | x | $\\pi/2$ | x1. | 1 | 0 |\n",
    "| $x_{2}$ | y | $\\pi/3$ | x2. | 0 | 1 |\n",
    "| $v_{1}$ | $\\sin(x1)$ | 1 | $\\cos(x1)*\\dot{x}1$ | 0 | 0 |\n",
    "| $v_{2}$ | cos(x2) | 0.5 | $\\sin(x2)*\\dot{x}_{2}$ | 0 | $\\sqrt{3}/2$ |\n",
    "| ${v}_{3}$ | $v_{1} - v_{2}$ | 0.5 |  $\\dot{v}_{1} - \\dot{v}_{2}$ | 0 | $\\sqrt{3}/2$  |\n",
    "| ${v}_{4}$ | ${v}_{3}^2$ | 1/4 | $2{v}_{3}  \\dot{v}_{3}$ |0  |$\\sqrt{3}/2$   | \n",
    "| ${v}_{5}$ | $-{v}_{4}$ | -1/4 | $-\\dot{v}_{4}$ | 0 |- $\\sqrt{3}/2$ | \n",
    "| ${v}_{6}$| $exp({v}_{5})$ | $exp(-1/4)$ | $exp({v}_{5})*\\dot{v}_{5}.$  | 0 | $\\exp(-1/4)$ * $(-\\sqrt{3}/2)$  | \n",
    "\n",
    "#### The Forward mode\n",
    "We work from the inside out, starting with what we want to evaluate \n",
    "$x= \\left(\\dfrac{\\pi}{2}, \\dfrac{\\pi}{3}\\right)$, then build out the actual function at each step subsituting the derivative of the inner functions  in the chain rule. \n",
    " The function f(x,y) is composed of elementary functions which we know the derivative of, we compute the derivative of the elementary functions then \n",
    " use the chain rule to build up to the larger function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-88d55838-8c26-4be9-9acb-5bed8de0652d",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "# How to Use DeriveMeCrazy Auto_diff:\n",
    "## Importing: \n",
    "\n",
    "#### Direct import:\n",
    "Users will be able to clone the project off of github and run it locally.\n",
    "Once they have the code in their local env, they will be able to install with pip and import the different classes implemented. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-3cb8397c-a83c-4ae6-957d-90151722c04a",
    "deepnote_cell_type": "code",
    "execution_millis": 3056,
    "execution_start": 1603401670993,
    "output_cleared": true,
    "source_hash": null,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-ce206903-22cb-42bc-9383-9fc1fa1e28ff",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Demo\n",
    "\n",
    "We include a basic demo below, including how to import, create an object, and use `AutoDiff` for calculating derivatives in an arithmetic operation and a trignometric operation. A more detailed demo for more types of operations is included in `docs/demo.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-07b8abd4-a8f6-4028-8d15-67e47a895240",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00005-4abae248-4140-4aa9-8157-bcbc71a00ba8",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import auto_diff_pkg.AutoDiff as AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-b1cf517f-e40c-4995-82eb-dbf1fd808914",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Creating an `AutoDiff` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00007-72a8766f-5841-4ea5-a5bd-fb864affd6f4",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ad1 = AD.AutoDiff(5.0)\n",
    "ad2 = AD.AutoDiff(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-3b2c1453-c028-4e50-8c36-745c41ca6b3c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00010-84f592ae-e02c-49fb-88c0-ec6982e48d06",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ad3 = ad1 * ad2\n",
    "\n",
    "print('value: {}'.format(ad3.val))\n",
    "print('derivative: {}'.format(ad3.der))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-029f3a69-e72e-4c1d-8e4a-9a5812c6f9c9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Trignometric operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-fbe9cd78-8d72-49a1-b612-a11323565775",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ad4 = AD.sin(ad1)\n",
    "\n",
    "print('value: {}'.format(ad4.val))\n",
    "print('derivative: {}'.format(ad4.der))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-7a6e6c6a-d0cf-4008-8d4f-dce6592ab52f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Software Organization\n",
    "- Other than the setup related files, we will have a directory for each of the following:\n",
    "\n",
    "- `auto_diff_pkg/` - all source code \n",
    "    - `__init__.py`\n",
    "    - `AutoDiff.py`\n",
    "    - `ReverseAutoDiff.py`\n",
    "- `docs/` - documentation and usage examples\n",
    "    - `demo.ipynb`\n",
    "    - `milestone1.ipynb`\n",
    "    - `milestone2_progress.ipynb`\n",
    "    - `milestone2.ipynb`\n",
    "    - `documentation.ipynb`\n",
    "- `tests/` - test suites files \n",
    "    - `test_AutoDiff.py`\n",
    "- `.travis.yml`\n",
    "- `README.md`\n",
    "- `requirements.txt`\n",
    "- `setup.py`\n",
    "- `LICENSE`\n",
    " \n",
    "What modules do you plan on including? What is their basic functionality?\n",
    "- Ideally we want our module to be as independent as possible and therefore we will try to rely only on one dependancy - `numpy` for elementary operations. \n",
    "This will allow us to utilize the basic data structures and math operations we don't need to overload, including trigonometric functions, exponentiations, and square roots. \n",
    "- In order to run the test suite we also used `pytest` and `codecov` packages \n",
    "\n",
    "Where will your test suite live? Will you use TravisCI? CodeCov?\n",
    "- Our tests will live under the `tests/` subdirectory of our project. \n",
    "- Test suite is integrated with Travis-ci so that all tests are running with every push done to the repository \n",
    "- Codecov is also integrated so that testing code coverage is evaluated with every new build done on travis. \n",
    "- The repository README file contains both testing and coverage status tags that are pulling information from Travis and CodeCov\n",
    "\n",
    "How will you distribute your package?\n",
    "- We will use Github to distribute our package. Users can clone the repository and is set up in such a way that will allow them to install the package directly with a pip command. This will also validate our package with the current python standards for software distribution.\n",
    "- Our code will also be available on github for direct cloning for those who wish to extend our code or simply prefer to not use pip.\n",
    "\n",
    "How will you package your software? Will you use a framework? If so, which one and why? If not, why not?\n",
    "- After reviewing several python framework, we have decided to only follow the requirements for PyPI packaging and distribution. \n",
    "- This is a well established and supported process.\n",
    "There is no constraints on the testing or source code control tools we use. \n",
    "Is is simple enough so that we are sure this will not impact our project in terms of overhead. \n",
    "\n",
    "Other considerations:\n",
    "- We also took into account the fact that our team members are for the most part relatively new to python packaging and so we wanted to make sure we make choices that are appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-dcf8c171-28fc-4c21-b42c-c84880c33f35",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "# Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-40de5ea2-2534-4e17-8411-1484d7761a77",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Core data structures:\n",
    "- Our core data structure is an `AutoDiff` object consisting of 2 float variables for storing values and derivatives. We recursively store values and derivatives for every elementary function.\n",
    "\n",
    "Core classes we will implement:\n",
    "- A class `AutoDiff` for elementary functions that takes the advantage of dunder methods to recursively calculate the intermediate values of the trace and stores the value and the derivative of an operation.\n",
    "- Within the same file in `AutoDiff.py` we have functions in the package that will perform the operations including trignometric functions, exponentiation, square root.\n",
    "- A class `AD_Trace` for the trace table and graph.\n",
    "\n",
    "Methods and name attributes our classes have:\n",
    "- Class`AutoDiff`: \n",
    "    - `__init__`\n",
    "        - `self.val` (value of an AutoDiff object that will be updated per elementary operation)\n",
    "        - `self.der` (derivative of an AutoDiff object that will be updated per elementary operation)\n",
    "    - `__neg__`\n",
    "    - operator overload for all elementary functions, including `__add__`, `__radd__`, `__sub__`, `__rsub__`, `__mul__`, `__rmul__`, `__truediv__`, `__rtruediv__`, `__pow__`, `__rpow__`\n",
    "    - `__str__`\n",
    "    - Trignometric, exponentiation, square root functions that take an AutoDiff object as argument: `log`, `exp`, `sin`, `cos`, `tan`, `sqrt`\n",
    "- Class `ReverseAutoDiff`:\n",
    "    - `__init__`\n",
    "        - `self.graph`\n",
    "        - `self.variables` (variables for multivariable differentiations)\n",
    "        - `self.functions` (corresponding functions for multifunction)\n",
    "- Class `ReverseADNode`:\n",
    "    - `__init__`\n",
    "        - `self.value` (current value of the node)\n",
    "        - `self.children` (children of this reverse mode operations)\n",
    "        - `self.grad_value` (current value of the gradient)\n",
    "        - `self.op` (associated elementary operation)\n",
    "\n",
    "External dependencies we rely on:\n",
    "- `numpy` for evaluating the elementary operations.\n",
    "- `pytest` for evaluating our test suite.\n",
    "- `notebook` for enabling the interactive demo.\n",
    "\n",
    "\n",
    "How will you deal with elementary functions like sin, sqrt, log, and exp (and all the others)?\n",
    "- Arithmatic operations such as `add`, `sub`, `mul`, `truediv`, `pow` are implemented as dunder methods in `AutoDiff` class. The reverse dunder methods are also implemented.\n",
    "- Elementary functions like like sin, sqrt, log, and exp will be done outside the class `AutoDiff` but still within our package module.\n",
    "This is due to the fact that these math functions are not defined in Python out of the box (they need to be pulled from Numpy, Math etc.), \n",
    "and we wanted to make sure we utilize Python's built in order of operations so we can avoid having to parse any function strings. \n",
    "These functions will operate on both `AutoDiff` objects as well as on other types such as int, float etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension: Reverse Mode\n",
    "Our initial plan for the first two milestones was to implement backpropagation for our extenstion. However, as a team we decided that it would be more beneficial if the program instead implements the reverse mode. In comparison to the forward mode, where the final product is the Jacobian-vector product $Jp$, the final product of the reverse mode is $J^Tp$. By implementing the reverse mode, users of our package will be able to get the most efficient solution, regardless of the number of $m$ seed vectors and $n$ functions. When the user encounters a system where $n >> m$, they can use the forward mode implementation, and when they have a system where $m >> n$, they can use the reverse mode implementation.\n",
    "\n",
    "The reverse mode starts by taking a forward pass through the the elementary functions, storing the partial derivatives along the way (without evaluation the chain rule). We then iterate from the end of the trace, multiplying the current partial derivative with the previous, i.e $v_N=\\frac{\\delta f}{\\delta v_N}$, $v_{N-1}=\\frac{\\delta f}{\\delta v_N}\\frac{\\delta v_N}{\\delta v_{N-1}}$, and so on. The when a partial derivative has multiple children in the trace, their sum is taken instead. The structure and implementation code for doing so is given above in conjunction with the forward mode specifics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-f366ecdf-a08d-4224-9760-5d5f9d0ee236",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# ------ ADDRESS THIS BEFORE SUBMITTING ---------------\n",
    "#### AutoDiff Class Extensions\n",
    "In order to make the AutoDiff class more useful overall, we plan to implement a few more dunder methods into the main class. As of now, our plan is to add methods for:\n",
    "- __repr__\n",
    "- comparators __eq__,__lt__,__gt__, etc.\n",
    "\n",
    "#### Additional functions:\n",
    "We would like to include additional functions that were not yet implemented such as sinh, cosh, tanh. \n",
    "\n",
    "#### Supporting Graph representation \n",
    "We would like like to be able to present the graph of operations for forward mode. \n",
    "\n",
    "\n",
    "#### Supporting Display of Evaluation Table \n",
    "\n",
    "#### Implementation of Root Finder \n",
    "We can, if time allows, add this as a feature of our module, similar to what was demonstrated in our demo notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00017-5acb2bcd-1d21-48fe-814f-6294045ac5b3",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "cca98a6a-82e2-45d7-8fa5-00d0fd6eb7fe",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
